/*
This file is part of MarFS, which is released under the BSD license.


Copyright (c) 2015, Los Alamos National Security (LANS), LLC
All rights reserved.

Redistribution and use in source and binary forms, with or without modification,
are permitted provided that the following conditions are met:

1. Redistributions of source code must retain the above copyright notice, this
list of conditions and the following disclaimer.

2. Redistributions in binary form must reproduce the above copyright notice,
this list of conditions and the following disclaimer in the documentation and/or
other materials provided with the distribution.

3. Neither the name of the copyright holder nor the names of its contributors
may be used to endorse or promote products derived from this software without
specific prior written permission.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT,
INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE
OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF
ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

-----
NOTE:
-----
MarFS uses libaws4c for Amazon S3 object communication. The original version
is at https://aws.amazon.com/code/Amazon-S3/2601 and under the LGPL license.
LANS, LLC added functionality to the original work. The original work plus
LANS, LLC contributions is found at https://github.com/jti-lanl/aws4c.

GNU licenses can be found at <http://www.gnu.org/licenses/>.


From Los Alamos National Security, LLC:
LA-CC-15-039

Copyright (c) 2015, Los Alamos National Security, LLC All rights reserved.
Copyright 2015. Los Alamos National Security, LLC. This software was produced
under U.S. Government contract DE-AC52-06NA25396 for Los Alamos National
Laboratory (LANL), which is operated by Los Alamos National Security, LLC for
the U.S. Department of Energy. The U.S. Government has rights to use,
reproduce, and distribute this software.  NEITHER THE GOVERNMENT NOR LOS
ALAMOS NATIONAL SECURITY, LLC MAKES ANY WARRANTY, EXPRESS OR IMPLIED, OR
ASSUMES ANY LIABILITY FOR THE USE OF THIS SOFTWARE.  If software is
modified to produce derivative works, such modified software should be
clearly marked, so as not to confuse it with the version available from
LANL.

THIS SOFTWARE IS PROVIDED BY LOS ALAMOS NATIONAL SECURITY, LLC AND CONTRIBUTORS
"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,
THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
ARE DISCLAIMED. IN NO EVENT SHALL LOS ALAMOS NATIONAL SECURITY, LLC OR
CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT
OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING
IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY
OF SUCH DAMAGE.
*/


// _GNU_SOURCE defines pthread_timedjoin_np(), and pthread_tryjoin_np()
// #define _GNU_SOURCE
#include <pthread.h>
#include <signal.h>             // pthread_kill()

// attach a custom prefix to log messages generated by this module
#ifdef LOG_PREFIX
#  undef LOG_PREFIX
#endif
#define LOG_PREFIX "obj_stream"
#include "logging.h"

#include "common.h"
#include "object_stream.h"
#include "marfs_locks.h"

#include <stdlib.h>
#include <unistd.h>
#include <string.h>

#include <errno.h>


void stream_reset(ObjectStream* os, uint8_t preserve_os_written);


// With the advent of the DAL and stream_del(), it's starting to make sense
// to move this aws4c-specific initialization out of marfs_open(), so that
// (a) we can limit it to OBJECT-DAL-related I/O only, and (b) we can also
// use it to set up stream_del().  In all these cases, we'll now want the
// DAL init function to receive a file-handle, instead of just a pointer to
// the OS inside the file-handle.  That way, when the GC utility is
// deleting objects through the DAL, it can just create a file-handle, fill
// out the path-info, and do everything else through the DAL.
//
// The unused argument is so we can match the format the DAL_OP will expand
// into, in init_data(), in the case of a non-DAL build.

int stream_init(void* os_void, void* null_void, void* fh_void) {
   TRY_DECLS();

   MarFS_FileHandle* fh   = (MarFS_FileHandle*)fh_void;
   PathInfo*         info = &fh->info;
   ObjectStream*     os   = &fh->os;
   IOBuf*            b    = &os->iob;

   // Configure a private AWSContext, for this request
   AWSContext* ctx = aws_context_clone();
   if (ACCESSMETHOD_IS_S3(info->pre.repo->access_method)) { // (includes S3_EMC)

      // install the host and bucket
      s3_set_host_r(info->pre.host, ctx);
      LOG(LOG_INFO, "host   '%s'\n", info->pre.host);
      // fprintf(stderr, "host   '%s'\n", info->pre.host); // for debugging pftool

      s3_set_bucket_r(info->pre.bucket, ctx);
      LOG(LOG_INFO, "bucket '%s'\n", info->pre.bucket);
   }

   if (info->pre.repo->access_method == ACCESSMETHOD_S3_EMC) {
      s3_enable_EMC_extensions_r(1, ctx);

      // For now if we're using HTTPS, I'm just assuming that it is without
      // validating the SSL certificate (curl's -k or --insecure flags). If
      // we ever get a validated certificate, we will want to put a flag
      // into the MarFS_Repo struct that says it's validated or not.
      if ( info->pre.repo->ssl ) {
         s3_https_r( 1, ctx );
         s3_https_insecure_r( 1, ctx );
      }
   }
   else if (info->pre.repo->access_method == ACCESSMETHOD_SPROXYD) {
      s3_enable_Scality_extensions_r(1, ctx);
      s3_sproxyd_r(1, ctx);

      // For now if we're using HTTPS, I'm just assuming that it is without
      // validating the SSL certificate (curl's -k or --insecure flags). If
      // we ever get a validated certificate, we will want to put a flag
      // into the MarFS_Repo struct that says it's validated or not.
      if ( info->pre.repo->ssl ) {
         s3_https_r( 1, ctx );
         s3_https_insecure_r( 1, ctx );
      }
   }

   if (info->pre.repo->security_method == SECURITYMETHOD_HTTP_DIGEST) {
      s3_http_digest_r(1, ctx);
   }

   // install custom context
   aws_iobuf_context(b, ctx);

   return 0;
}


// After cancelling the thread, we can be pretty confident that the thread
// is not still inside some aws4c function.  However, the context may still
// think it is "inside".  Some of the aws4c functions protect themselves
// against changing context-settings while they are "inside" operations.
// And those functions may be run as part of cleanup, after a thread is
// cancelled.  Turn "inside" off, so the clean-up functions
// (e.g. aws_reset_iobuf_hard() in marfs_release()) won't fail an assertion
// in libaws4c.

void cancel_thread(ObjectStream* os) {
   int rc = pthread_cancel(os->op);
   if (rc) {
      LOG(LOG_ERR, "cancellation failed (%s), killing thread\n",
          strerror(errno));
      pthread_kill(os->op, SIGKILL);
      LOG(LOG_INFO, "killed thread\n");
   }
   else {
      LOG(LOG_INFO, "cancelled\n");
   }

   os->flags |= OSF_CANCELED;

   /* see NOTE, above */
   if (os->iob.context) {
      LOG(LOG_INFO, "resetting 'inside' flag in AWSContext\n");
      os->iob.context->inside = 0;
   }
}



// ---------------------------------------------------------------------------
// stream_wait()
//
// Wait until thread completes (and return immediately when that happens).
// Loop is necessary to handle the case where another thread is also trying
// to join (in which case, our join gets EINVAL).  This would happen, I
// think, in the case of dup'ed marfs file-handles.
//
// In the event of a failed join, we now forcibly cancel the thread.  This
// simplifies stream_sync(), which can now assume that stream_wait() always
// succeeds.
//
// NOTE: The return from pthread_timedjoin_np() is not the same as the
//     retval from the thread!  We return 0 if the thread was joined
//     successfully.  You need to check errno for e.g. curl error-codes
//     returned from GET/PUT.
//
// NOTE: This supports fuse flush.  Fuse flush is not the same as fflush().
//     Fuse flush means to wait until the all possible reads/writes are
//     completed, such that no more errors can be generated on this stream.
//
// NOTE: Can't use pthread_timedjoin_np(), without #define _GNU_SOURCE.
//     Not sure I want to do that.
//
// TBD: compare accumulated wait with a max-timeout limit, that could be
//     provided in e.g. OpenStream.timeout_sec, representing a maximum time
//     that a stream was allowed to be unresponsive.
//
// ---------------------------------------------------------------------------

static
int stream_wait(ObjectStream* os) {

   // sync
   if (os->flags & OSF_JOINED)
      return 0;

   static const size_t  default_timeout = 20;
   size_t timeout_sec = (os->timeout ? os->timeout : default_timeout);

   struct timespec timeout;
   if (clock_gettime(CLOCK_REALTIME, &timeout)) {
      LOG(LOG_ERR, "failed to get timer '%s'\n", strerror(errno));
      return -1;                // errno is set
   }
   timeout.tv_sec += timeout_sec;


   int   rc;
   void* thread_retval = 0;
   while (1) {

      // check whether thread has returned.  Could mean a curl error, an S3
      // protocol error, or server flaking out.  Successful return will
      // have saved retval in os->op_rc, so we don't have to return it.
      rc = pthread_timedjoin_np(os->op, &thread_retval, &timeout);
      LOG(LOG_INFO, "pthread_timedjoin_np returned %d (errno=%d)\n", rc, errno);
      LOG(LOG_INFO, "thread retval (via os->op_rc) %d\n", os->op_rc);

      if (! rc) {
         os->flags |= OSF_JOINED;
         break;                 // joined
      }
      else if (rc == ESRCH) {
         os->flags |= OSF_JOINED;
         break;                 // no-such thread  (treat as success)
      }
      else if (rc == EINVAL) {
         // EINVAL == another thread is also trying to join (?)  timed-wait
         // will just return immediately again, and we'll turn into a
         // busy-wait.  Add an explicit sleep here.  [Alternatively, here's
         // another idea: If multiple threads are waiting on this thread, then
         // that implies the fuse handle was dup'ed.  In that case, if we
         // return success, fuse might still wait until all threads have
         // flushed?  I don't trust my knowledge of fuse enough to depend on
         // that.]
         sleep(1);
      }
      else if (rc == ETIMEDOUT) {
         errno = rc;
         break;             // timed-out
      }
      else {
         errno = rc;
         break;             // error
      }
   }

   // if we failed to join, then cancel the thread
   if (! (os->flags & OSF_JOINED)) {
      LOG(LOG_INFO, "join failed: %s\n", strerror(errno));
      cancel_thread(os);
      pthread_tryjoin_np(os->op, &thread_retval);
   }

   return 0;
}



// ---------------------------------------------------------------------------
// GET / PUT thread
// ---------------------------------------------------------------------------

// This just allows us to use AWS_CHECK(), returning an int, where s3_op
// has to return void*.  We return 0 for success, non-zero for failure.
//
// Any errors from curl, S3, etc, will be found in os->iob->result.
// If there are any, we'll be returning non-zero.
//
// TBD: If we return non-zero on a GET, writefunc will be stranded waiting
//      for us.  Now, it will time-out on its sem_wait and report an error.
//      Therefore, we should return zero on all legitimate cases.

int s3_op_internal(ObjectStream* os) {
   IOBuf*        b  = &os->iob;
   __attribute__ ((unused)) AWSContext*   ctx = b->context;

   // run the GET or PUT
   int is_get = (os->flags & OSF_READING);
   if (is_get) {
      LOG(LOG_INFO, "GET  '%s/%s/%s'\n",
          (ctx ? ctx->S3Host : "*"),  (ctx ? ctx->Bucket : "*"), os->url);
      AWS4C_CHECK1( s3_get(b, os->url) ); /* create empty object with user metadata */
   }
   else {
      LOG(LOG_INFO, "PUT  '%s/%s/%s'\n",
          (ctx ? ctx->S3Host : "*"),  (ctx ? ctx->Bucket : "*"), os->url);
      // If you are getting errors here, the comments above the "#if
      // ((LIBCURL_VERSION ...", in stream_sync(), *might* be relevant.
      AWS4C_CHECK1( s3_put(b, os->url) ); /* create empty object with user metadata */
   }


   // s3_get with byte-range can leave streaming_writefunc() waiting for a
   // curl callback that never comes.  This happens if there is still
   // writable space in the buffer, when the last bytes in the request are
   // processed.  This can happen because caller (e.g. fuse) may ask for
   // more bytes than are present, and provide a buffer big enough to
   // receive them.  Our stream_get() now avoids requests that exceed
   // file-size, and stream_sync() knows how to quit, whether user received
   // all the requested data or not.
   if (is_get && (b->code == 206)) {

      // should we do something with os->iob_full?  set os->flags & EOF?
      LOG(LOG_INFO, "GET complete\n");
      os->flags |= OSF_EOF;
      POST(&os->iob_full);
      return 0;
   }
   else if (AWS4C_OK(b) ) {
      LOG(LOG_INFO, "%s complete\n", ((is_get) ? "GET" : "PUT"));
      return 0;
   }
   //   else if ((curl_version_info(CURLVERSION_NOW)->version_num < 0x072d00)
   //            && (b->code == 0)) {
   //      // NOTE: Newer versions of Scality (5.0.4?) do not return status-codes
   //      //     that libcurl 7.19.7 can parse correctly.  In that case, we see
   //      //     b->code==0.  We're going to call this a success for the request.
   //      //     stream_sync will still examine whether
   //      LOG(LOG_INFO, "%s complete (libcurl allowance)\n", ((is_get) ? "GET" : "PUT"));
   //      return 0;
   //   }

   LOG(LOG_ERR, "CURL ERROR: %lx %d '%s'\n", (size_t)b, b->code, b->result);
   return -1;
}

// this runs as a separate thread, so that stream_open() can return
// NOTE: If you compile w/ logging disabled, then 'b' is unused 
void* s3_op(void* arg) {
   ObjectStream* os = (ObjectStream*)arg;
   __attribute__ ((unused)) IOBuf* b  = &os->iob;

   if ((os->op_rc = s3_op_internal(os)))
      LOG(LOG_ERR,  "failed (%s) %d (%s) '%s'\n",
          os->url, os->op_rc, curl_easy_strerror(os->op_rc), b->result);
   else
      LOG(LOG_INFO, "done (%s)\n", os->url);

   return os;
}


// ---------------------------------------------------------------------------
// PUT (write)
//
// This is installed as the "readfunc" which is called by curl, whenever it
// needs more data for a PUT.  (From curl's perspective, it's a "read" of
// our PUT data.)  This function is invoked in its own thread by curl.  We
// synchronize with stream_put(), where a user has more data to be added to
// a stream.  The user's buffer may be larger than the size of the buffer
// curl gives us to fill, in which case we self-enable, so the next
// call-back can happen immediately.  Finally, stream_close gives us an
// empty buffer (with length 0), which tells us to signal EOF to curl,
// which we do by returning 0.
//
// NOTE: We now also allow stream_abort to send length 0, but in this case
//     with a buffer that is just ((char*)1.  We take this as a signal to
//     return CURL_READFUNC_ABORT.
// ---------------------------------------------------------------------------

size_t streaming_readfunc(void* ptr, size_t size, size_t nmemb, void* stream) {
   LOG(LOG_INFO, "entry\n");

   IOBuf*        b     = (IOBuf*)stream;
   ObjectStream* os    = (ObjectStream*)b->user_data;
   size_t        total = (size * nmemb);
   LOG(LOG_INFO, "(%08lx) curl buff %ld\n", (size_t)os, total);

   // wait for producer to fill buffers
   WAIT(&os->iob_full);
   LOG(LOG_INFO, "(%08lx) avail-data: %ld\n", (size_t)os, b->avail);

   // maybe we were requested to quit or abort?
   if (b->write_count == 0) {
      // called by stream_sync()
      LOG(LOG_INFO, "(%08lx) got EOF\n", (size_t)os);
      POST(&os->iob_empty); // polite
      return 0;
   }
   else if (b->first->buf == (char*)1) {
      // called by stream_abort()
      LOG(LOG_INFO, "(%08lx) got ABORT\n", (size_t)os);
      POST(&os->iob_empty); // polite
      return CURL_READFUNC_ABORT;
   }

   // move producer's data into curl buffers.
   // (Might take more than one callback)
   size_t move_req = ((total <= b->avail) ? total : b->avail);
   size_t moved    = aws_iobuf_get_raw(b, (char*)ptr, move_req);

   // track total size
   os->written += moved;
   LOG(LOG_INFO, "(%08lx) moved %ld  (total: %ld)\n", (size_t)os, moved, os->written);

   if (b->avail) {
      LOG(LOG_INFO, "(%08lx) iterating (avail: %ld)\n", (size_t)os, b->avail);
      POST(&os->iob_full);  // next callback is pre-approved
   }
   else {
      LOG(LOG_INFO, "(%08lx) done with buffer (total written %ld)\n", (size_t)os, os->written);
      POST(&os->iob_empty); // tell producer that buffer is used
   }

   return moved;
}

// Hand <buf> over to the streaming_readfunc(), so it can be added into
// the ongoing streaming PUT.  You must call stream_open() first.
//
// NOTE: Doing this a little differently from the test_aws.c (case 12)
//       approach.  We're forcing *synchronous* interaction with the
//       readfunc, because we don't want caller's <buf> to go out of scope
//       until the readfunc is finished with it.
//
int stream_put(ObjectStream* os,
               const char*   buf,
               size_t        size) {

   static const uint16_t default_timeout = 20; /* totally made up out of thin air */
   uint16_t timeout_sec = (os->timeout ? os->timeout : default_timeout);

   LOG(LOG_INFO, "(%08lx) entry\n", (size_t)os);
   if (! (os->flags & OSF_OPEN)) {
      LOG(LOG_ERR, "(%08lx) %s isn't open\n", (size_t)os, os->url);
      errno = EBADF;
      return -1;
   }
   if (! (os->flags & OSF_WRITING)) {
      LOG(LOG_ERR, "(%08lx) %s isn't open for writing\n", (size_t)os, os->url);
      errno = EBADF;
      return -1;
   }
   IOBuf* b = &os->iob;         // shorthand

#if 0
   // QUESTION: Does it improve performance to copy the caller's buffer,
   //    so we can return immediately?
   //
   // ANSWER: No.

   // readfunc done with IOBuf?
   LOG(LOG_INFO, "(%08lx) waiting %ds for IOBuf\n", (size_t)os, timeout_sec); 
   SAFE_WAIT(&os->iob_empty, timeout_sec, os);
   //   SAFE_WAIT_KILL(&os->iob_empty, timeout_sec, os);

   static size_t tmp_size = 0;
   static char*  tmp_buf = NULL;
   if (size > tmp_size) {
      if (tmp_size)
         free(tmp_buf);
      tmp_size = size;
      tmp_buf = (char*) malloc(size);
      if (! tmp_buf) {
         errno = ENOMEM;
         return -1;
      }
   }
   memcpy(tmp_buf, buf, size);
   
   // install buffer into IOBuf
   aws_iobuf_reset(b);          // doesn't affect <user_data>
   aws_iobuf_append_static(b, tmp_buf, size);
   LOG(LOG_INFO, "(%08lx) installed buffer (%ld bytes) for readfn\n", (size_t)os, size);

   // let readfunc move data
   POST(&os->iob_full);

#else
   // install buffer into IOBuf
   aws_iobuf_reset(b);          // doesn't affect <user_data>
   aws_iobuf_append_static(b, (char*)buf, size);
   LOG(LOG_INFO, "(%08lx) installed buffer (%ld bytes) for readfn\n", (size_t)os, size);

   // let readfunc move data
   POST(&os->iob_full);

   // readfunc done with IOBuf?
   LOG(LOG_INFO, "(%08lx) waiting %ds for IOBuf\n",
       (size_t)os, timeout_sec);
   SAFE_WAIT(&os->iob_empty, timeout_sec, os);
   //   SAFE_WAIT_KILL(&os->iob_empty, timeout_sec, os);

#endif

   LOG(LOG_INFO, "(%08lx) buffer done\n", (size_t)os); // readfunc done with IOBuf?
   return size;
}


// ---------------------------------------------------------------------------
// GET (read)

// curl calls streaming_writefunc with some incoming data on a GET, which
// we are supposed to "write" somewhere.  (From curl's perspective, it's a
// "write" of data to our GET buffer.)  We interact with stream_get(), to
// write our data into a buffer that a caller provided to stream_get().
//
// streaming_writefunc() is more complex than streaming_readfunc(), because we
// don't return to curl until we have exhausted curl's buffer.  [because
// doc says curl treats anything less than that as an error.  Should test
// that.]  That means is may require multiple calls to stream_get(), before
// we can write all of curl's buffer.
//
// If it so happens that the object is not-bigger-than the buffer, or the
// GET is issued with a byte-range not-bigger-than the buffer, we will fill
// the buffer.
//
// As in the case of stream_put() plus readfunc(), we're guessing that the
// buffer sizes presented to stream_get() by users will tend to be much
// larger than the 16k buffer presented to streaming_writefunc() by curl,
// so it's probably better to use the buffer provided to stream_get() as
// the basis of the shared IOBuf, rather than using the buffer provided by
// curl to streaming_writefunc().
//
// NOTE: I think we're not in danger of deadlocking if someone closes
//       prematurely, because ObjectStream has the pthread that is doing
//       the GET.  It can always just kill that thread, as part of a close,
//       which (I think) ought to end curl's expectaions about the
//       write-function.
//
//       For extra nice-ness, for the case where we are being closed before
//       the object has been entirely read, we could add a flag to check
//       (e.g. CLOSING).  Then stream_close() could set that (in
//       ObjectStream), and post iob_full, so we'd get a chance to iterate,
//       see the flag, and return 0 to curl.  I expect this would allow the
//       thread to complete naturally (with a curl error-code), instead of
//       requiring being killed.
//
// NOTE: After a little experimentation, it looks like curl never calls us
//       with size 0 to indicate EOF.  Instead, the GET thread will just
//       return.  In the case where the user reads only part of a file,
//       even that won't happen.  Therefore, I think we (or stream_get)
//       will need to count the bytes going past.
//
//       No, it's worse than that.  If the byte-range is beyond EOF, then
//       no write-function gets called
//
// ---------------------------------------------------------------------------

// After EOF, fuse makes a callback to marfs_read() with a byte-range
// beyond the end of the object.  In that case, curl never calls
// streaming_writefunc(), so stream_get() is stuck waiting for iob_full.
// This function can detect that case, because the Content-length returned
// in the header will be zero.
size_t streaming_writeheaderfunc(void* ptr, size_t size, size_t nitems, void* stream) {

   size_t result = aws_headerfunc(ptr, size, nitems, stream);

   // if we've parsed content-length from the response-header, and length
   // was zero, then there will be no callback to streaming_writefunc().
   // Therefore, streaming_writefunc() will never post iob_full, and
   // stream_get() will wait forever (or until it times out).  We have
   // knowledge that this is happening, so we can post iob_full ourselves,
   // and let stream_get() proceed.
   if ( !strncmp( ptr, "Content-Length: ", 15 )) {
      IOBuf*        b     = (IOBuf*)stream;
      ObjectStream* os    = (ObjectStream*)b->user_data;
      if (!b->contentLen) {
         LOG(LOG_INFO, "detected EOF\n"); // readfunc done with IOBuf?
         os->flags |= OSF_EOF;                            // (or error)
         POST(&os->iob_full);
      }
      else
         LOG(LOG_INFO, "content-length (%ld) is non-zero\n", b->contentLen);
   }
   return result;
}


size_t streaming_writefunc(void* ptr, size_t size, size_t nmemb, void* stream) {

   IOBuf*        b     = (IOBuf*)stream;
   ObjectStream* os    = (ObjectStream*)b->user_data;
   size_t        total = (size * nmemb);
   LOG(LOG_INFO, "curl-buff %ld\n", total);

   // wait for user-buffer, supplied to stream_get()
   WAIT(&os->iob_empty);
   LOG(LOG_INFO, "user-buff %ld\n", (b->len - b->write_count));

   // maybe we were requested to quit?
   if (b->first == NULL) {
      // called by stream_sync()
      LOG(LOG_INFO, "got QUIT\n");
      POST(&os->iob_full);
      return 0;           // op-thread will fail with CURLE_WRITE_ERROR (?)
   }

   // check for EOF on the object
   if (! total) {
      os->flags |= OSF_EOF;
      POST(&os->iob_full);
      LOG(LOG_INFO, "EOF done\n");
      return 0;
   }

   size_t avail    = total;     /* availble for reading from curl-buffer */
   char*  dst      = ptr;
   size_t writable = (b->len - b->write_count); /* space for writing in user-buffer */
   while (avail) {

      LOG(LOG_INFO, "iterating: writable=%ld, readble=%ld\n", writable, avail);

      // if user-buffer is full, wait for another one
      if (! writable) {
         LOG(LOG_INFO, "user-buff is full\n");
         os->flags |= OSF_EOB;     // need fresh buffer from stream_get()
         POST(&os->iob_full);
         WAIT(&os->iob_empty);

         // maybe we were requested to quit?
         if (b->first == NULL) {
            // called by stream_sync()
            LOG(LOG_INFO, "got QUIT\n");
            POST(&os->iob_full);
            return 0;           // op-thread will fail with CURLE_WRITE_ERROR (?)
         }

         writable = (b->len - b->write_count);
         continue;
      }

      // move data to user's buffer
      size_t move = ((writable < avail) ? writable : avail);
      aws_iobuf_append(b, dst, move);

      avail    -= move;
      writable -= move;
      dst      += move;
   }

   // curl-buffer is exhausted.
   LOG(LOG_INFO, "copied all of curl-buff (writable=%ld)\n", writable);
   if (writable)
      POST(&os->iob_empty); // next curl-callback is pre-approved
   else {
      os->flags |= OSF_EOB;     // need fresh buffer from stream_get()
      POST(&os->iob_full);
   }
   return total;                /* to curl */
}



// Accept as much as <size>, from the streaming GET, into caller's <buf>.
// We may discover EOF at any time.  In that case, we'll return however
// much was actually read.  The next call-back (after EOF)
// will just short-circuit to return 0, signalling EOF to caller.
// 
// return -1 with errno, for failures.
// else return number of chars we get.
//
// NOTE: We need to be careful with aws_iobuf_reset(), because the iobuf
//     may already have had the response code parsed out of HTTP headers
//     and placed into b->code, and b->result.

ssize_t stream_get(ObjectStream* os,
                   char*         buf,
                   size_t        size) {

   static const uint16_t default_timeout = 20; /* totally made up out of thin air */
   uint16_t timeout_sec = (os->timeout ? os->timeout : default_timeout);

   IOBuf* b = &os->iob;     // shorthand

   LOG(LOG_INFO, "entry\n");
   if (! (os->flags & OSF_OPEN)) {
      LOG(LOG_ERR, "%s isn't open\n", os->url);
      errno = EBADF;
      return -1;
   }
   if (! (os->flags & OSF_READING)) {
      LOG(LOG_ERR, "%s isn't open for reading\n", os->url);
      errno = EBADF;
      return -1;
   }
   if (os->flags & OSF_EOF) {
      LOG(LOG_INFO, "already at EOF\n");
      return 0; // b->write_count;
   }
   os->flags &= ~(OSF_EOB);

   // We now create read-requests with a byte-range that is recorded in
   // OS.content_length.  There's no point letting you try to read more
   // than that, because it's never going to arrive.  Furthermore, you
   // would have to wait for a timeout, and marfs_read() would have to
   // figure out whether something went wrong, or you just got less than
   // you asked for.  The answer would be that you would've gotten less
   // than you asked for.  Simpler approach: reduce caller's request to the
   // constraints of the byte-range we requested.
   if (os->content_len
       && ((os->written + size) > os->content_len)) {
      LOG(LOG_INFO, "truncating size from %lu to %lu\n",
          size, os->content_len - os->written);
      size = os->content_len - os->written;
   }

   // The point of the reset is to pop off any previously-used buffers.
   // However, streaming_writeheaderfunc() may already have installed the
   // result code, parsed from headers in the response from the server.
   // Preserve those values, when resetting the IOBuf.
   aws_iobuf_reset_lite(b);          // doesn't affect <user_data>
   aws_iobuf_extend_static(b, (char*)buf, size);
   LOG(LOG_INFO, "got %ld-byte buffer for writefn\n", size);

   // let writefn move data
   POST(&os->iob_empty);

   // wait for writefn to fill our buffer
   LOG(LOG_INFO, "waiting %ds for writefn\n", timeout_sec);
   SAFE_WAIT(&os->iob_full, timeout_sec, os);
   //   SAFE_WAIT_KILL(&os->iob_full, timeout_sec, os);

   // writefn detected CURL EOF?
   if (os->flags & OSF_EOF) {
      LOG(LOG_INFO, "EOF is asserted\n");
   }
   if (os->flags & OSF_EOB) {
      LOG(LOG_INFO, "EOB is asserted\n");
   }

   os->written += b->write_count;
   LOG(LOG_INFO, "returning %ld (total=%ld)\n", b->write_count, os->written);
   return (b->write_count);
}



// ---------------------------------------------------------------------------
// OPEN (read/write)
//
// "chunked transfer-encoding" suppresses the header that specifies the
// total size of the object.  Instead, curl implements the chunked
// transfer-concoding, to indicate the size of each individual transfer.
//
// NOTE: We assume OS.url has been initialized
//
// TBD: POSIX allows you to open a file and then seek to an offset.  We
//      could do that here (where open just opens the stream, and then,
//      e.g. stream_get() skips through everything until your offset), but
//      that would probably have truly awful performance, on huge objects.
//      A better plan is to present the offset in the curl header, and let
//      the server skip to our offset, before sending anything.  However,
//      that implies that stream_open wouldn't actually complete until the
//      first stream_get is issued.  (Or, better yet, stream_open shouldn't
//      be called until the first stream_get is issued.)
//
// We now allow providing a content-length.  If it's non-zero (and we're
// writing), it will go into the curl request header.  Otherwise (if we're
// writing), we'll use chuunked-transfer- encoding.  Curl interacts with us
// differently, in the two cases.  When writing with a content-length, curl
// [7.45] makes no further callbacks to the readfunc, after it has received
// the specified number of chars.  This affects what stream_sync() should
// do.
//
// For reading, content_length is also used to communicate with stream_sync().
// In this case, when opening 
//       
// ---------------------------------------------------------------------------

int stream_open(ObjectStream* os,
                IsPut         put,
                size_t        chunk_offset,
                curl_off_t    content_length,
                uint8_t       preserve_os_written,
                uint16_t      timeout) {

   LOG(LOG_INFO, "%s  (timeout=%hu)\n", ((put) ? "PUT" : "GET"), timeout);

   curl_version_info_data* vers_data = curl_version_info(CURLVERSION_NOW);
   __attribute__ ((unused)) uint8_t curl_major = (vers_data->version_num >> 16) & 0xff;
   __attribute__ ((unused)) uint8_t curl_minor = (vers_data->version_num >>  8) & 0xff;
   __attribute__ ((unused)) uint8_t curl_patch =  vers_data->version_num        & 0xff;
   LOG(LOG_INFO, "runtime libcurl %u.%u.%u\n",
          curl_major, curl_minor, curl_patch);

   if (os->flags & OSF_OPEN) {
      LOG(LOG_ERR, "%s is already open (for %s)\n",
          os->url, ((os->flags & OSF_WRITING) ? "writing" : "reading"));
      errno = EEXIST;           // as though (O_CREAT|O_EXCL) ? Will NFS leave us alone?
      return -1;                // already open
   }
   if (os->flags) {
      if (os->flags & OSF_CLOSED) {
         LOG(LOG_INFO, "stream being re-opened with %s\n", os->url);
         stream_reset(os, preserve_os_written); // previously-used
      }
      else if (! (os->flags & ~OSF_RLOCK_INIT)) {
         LOG(LOG_INFO, "only flag was RLOCK_INIT\n");
      }
      else {
         LOG(LOG_ERR, "%s has flags asserted, but is not CLOSED\n", os->url);
         errno = EBADF;         // ???
         return -1;
      }
   }

   os->timeout = timeout;

   if (! preserve_os_written)
      os->written = 0;          // total read/written through OS

   // caller's open-flags, in case we need to close/repoen
   // (e.g. for Multi, or marfs_ftruncate())
   //
   //   os->open_flags = open_flags;

   // shorthand
   IOBuf* b = &os->iob;

   // readfunc/writefunc just get the IOBuf from libaws4c, but they need
   // the ObjectStream.  So IOBuf now has a pointer to allow this.
   b->user_data = os;

   // install copy of global default-context as per-connection context 
   if (! b->context) {
      LOG(LOG_INFO, "No context.  Cloning from defaults.\n");
      aws_iobuf_context(b, aws_context_clone());
   }
   AWSContext* ctx = b->context;

   os->content_len = content_length;
   if (put) {
      if (content_length)
         s3_set_content_length_r(content_length, ctx); // only used for PUT/POST
      else
         s3_chunked_transfer_encoding_r(1, ctx);       // only used for PUT/POST
   }
   else {
      s3_set_byte_range_r(chunk_offset, content_length, ctx);
   }


   // aws_iobuf_reset_lite(b);          // doesn't affect <user_data> or <context>
   aws_iobuf_reset(b);          // doesn't affect <user_data> or <context>

   if (put) {
      SEM_INIT(&os->iob_empty, 0, 0);
      SEM_INIT(&os->iob_full,  0, 0);
      aws_iobuf_readfunc(b, &streaming_readfunc);
   }
   else {
      SEM_INIT(&os->iob_empty, 0, 0);
      SEM_INIT(&os->iob_full,  0, 0);
      aws_iobuf_headerfunc(b, &streaming_writeheaderfunc);
      aws_iobuf_writefunc(b, &streaming_writefunc);
   }

   os->flags |= OSF_OPEN;
   if (put)
      os->flags |= OSF_WRITING;
   else
      os->flags |= OSF_READING;

   // thread runs the GET/PUT, with the iobuf in <os>
   LOG(LOG_INFO, "starting thread\n");
   if (pthread_create(&os->op, NULL, &s3_op, os)) {
      LOG(LOG_ERR, "pthread_create failed: '%s'\n", strerror(errno));
      errno = EIO;  // "something mysterious" went wrong with your write

      os->flags &= ~(OSF_OPEN);
      return -1;
   }

   return 0;
}


// ---------------------------------------------------------------------------
// SYNC
//
// This is like "flush" in the FUSE sense (i.e. no more I/O errors are
// possible), rather than "fflush" (i.e. wait for current buffers to be
// empty).  When stream_sync() returns, all I/O (ever) is completed on the
// stream.
//
// Fuse may call this before I/O is complete on the stream.  Therefore,
// we shouldn't just assume that failure to join means something is wedged.
//
// NOTE: Co-maintain with stream_sync().
//
// NOTE: This doesn't "close" the stream.  For that, you need
//       stream_close().  In fuse usage, we will first call stream_sync(),
//       then stream_close()
//
// NOTE: New approach to reads: each call to marfs_read() will create a
//       distinct GET request.  Therefore, the op should return (because of
//       EOF for the given byte-range), at the end of each call, so we
//       should simply join the thread.
//
//       For marfs_write(), writes start at offset zero, and are expected
//       to be contiguous.  Therefore, we set up a streaming readfunc, and
//       we must signal EOF when closing.
//
// UPDATE: marfs_read now minimizes GET requests (i.e. it assumes
//       contiguous reads until proven otherwise).  Therefore, it now *can*
//       leave streaming_writefunc waiting for another destination-buffer.
//       Therefore, we must signal for it to stop filling our buffer.  At
//       the end of a multi-file, the GET will still have all the
//       recovery-info we didn't ask for, but there could be other cases
//       where it has legit user-data we don't want (e.g.  user only
//       requested a small range, then closed).  When we signal
//       streaming_writefunc to quit early, it should return a wrong answer
//       to curl, in order to get curl to stop feeding it.  This means the
//       op-thread will return CURLE_WRITE_ERROR, which is okay.
//
// NOTE: If there are duplicated handles, fuse will call flush for each of
//       them.  That would imply that our pthread_join() may return EINVAL,
//       because another thread is already trying to join.  We do not currently
//       accomodate that.
//
// NOTE: If stream_sync() fails in e.g. marfs_write(), then marfs_write()
//       will report an error, and evntually the file will be closed
//       (either by fuse or by pftool).  marfs_release() will then call
//       stream_sync() again, as part of the normal close.  We should be
//       robust against that.
//
// ---------------------------------------------------------------------------

// wait for the S3 GET/PUT to complete
int stream_sync(ObjectStream* os) {

   void* retval;

   // fuse may call fuse-flush multiple times (one for every open stream).
   // but will not call flush after calling close().
   if (! (os->flags & OSF_OPEN)) {
      LOG(LOG_ERR, "%s isn't open\n", os->url);
      errno = EINVAL;            /* ?? */
      return -1;
   }

   // See NOTE, above, regarding the difference between reads and writes.
   if (os->flags & OSF_JOINED) {
      LOG(LOG_INFO, "already joined\n");
   }
   else if (! pthread_tryjoin_np(os->op, &retval)) {
      LOG(LOG_INFO, "op-thread joined\n");
      os->flags |= OSF_JOINED;
   }
   else {

      int cancel = 0;
      int wait   = 1;

      // If a stream_get/put timed-out waiting for their
      // writefunc/readfunc, then the locks are likely in an inconsistent
      // state.  [Either (a) the readfunc never posted iob_empty for
      // stream_put(), or (b) the writefunc never posted iob_full for
      // stream_get().]  In either case, the operation started by
      // stream_open() is declared a failure, and we shouldn't do any of
      // the normal cleanup stuff, like trying to write recovery-info, etc.
      // But we do need to terminate the thread before we return, because
      // if the writefunc/readfunc gets another callback, it will access
      // parts of the ObjectStream that are about to be deallocated.
      if (os->flags & OSF_TIMEOUT) {
         LOG(LOG_INFO, "flags indicate time-out\n");
         cancel = 1;
      }

      // In this case, the get/put timed-out, but it did so inside
      // SAFE_WAIT_KILL(), rather than SAFE_WAIT(), so the thread has
      // already been cancelled and joined.
      else if (os->flags & OSF_TIMEOUT_K) {
         LOG(LOG_INFO, "timed-out thread already killed\n");
         LOG(LOG_INFO, "op-thread returned %d\n", os->op_rc);
         wait = 0;
      }

      // Our installed version of libcurl is 7.19.7.  We are experimenting
      // with a custom-built libcurl based on 7.45.0.  We notice that the
      // latter does not call streaming_readfunc() again, if stream_open()
      // provided a content-length (i.e. the request had a content-length
      // header, rather than being chunked-transfer-encoded), and the full
      // number of chars matching the content-length header has been sent.
      // In such a case, stream_sync() shouldn't use "stream_put(..0)" to
      // get the readfunc to quit, because the readfunc isn't running.
      //
      // NOTE: This behavior may actually be present in earlier versions of
      //     libcurl, in which case, the VERSION_MINOR here should be
      //     adjusted downwards, toward 19.
      //
      // NOTE: curl_version_info_data.version_num is an unsigned int, that
      //     looks like this:
      // 
      //       ((uint8_t)major_version << 16)
      //     | ((uint8_t)minor_version <<  8)
      //     | ((uint8_t)patch_version)
      //
      // NOTE: We might be on libcurl >= 7.45 and still not satisfy all the
      //     tests in this expression.  For instance, we could have a
      //     write-stream with a content-length, which has not timed-out,
      //     yet is not yet complete.  In that case, it seems okay to
      //     continue onto the stream_put(0) of subsequent tests, to close
      //     this stream, because we *do* expect callbacks from libcurl, in
      //     that case.
      //
      else if ((curl_version_info(CURLVERSION_NOW)->version_num >= 0x072d00)
               && (os->flags & OSF_WRITING)
               && os->content_len
               && (os->content_len == os->written)) {

         LOG(LOG_INFO, "(wr) wrote content-len, no action needed (flags=0x%04x)\n",
             os->flags);
      }

      // signal EOF to readfunc
      else if (os->flags & OSF_WRITING) {
         LOG(LOG_INFO, "(wr) sending empty buffer (flags=0x%04x)\n", os->flags);
         if (stream_put(os, NULL, 0)) {
            LOG(LOG_ERR, "stream_put(0) failed\n");
            cancel = 1;
         }
      }

      // For reads, OS.content_len will match the byte-range used in the
      // stream_open().  For fuse/pftool, this is now always either to the
      // logical EOF in a packed, the logical EOD in a given chunk-object
      // in a Multi, or EOD in a Uni.  We also track the amount actually
      // accessed via stream_get().  Thus, if they have accessed all the
      // requested data, the request should complete successfully, without
      // further action.  (However, the thread won't necessarily have
      // joined above, because that may take time.)
      else if (os->written == os->content_len) {
         LOG(LOG_INFO, "(rd) read content-len, no action needed (flags=0x%04x)\n",
             os->flags);
      }

      // signal QUIT to writefunc
      else {
         LOG(LOG_INFO, "(rd) sending empty buffer (flags=0x%04x)\n", os->flags);
         if (stream_get(os, NULL, 0)) {
            LOG(LOG_ERR, "stream_get(0) failed\n");
            cancel = 1;
         }
      }


      // some cases above want to cancel the thread
      if (cancel) {
         LOG(LOG_INFO, "cancelling thread\n");
         cancel_thread(os);
      }

      // check whether thread has returned.  Could mean a curl
      // error, an S3 protocol error, or server flaking out.
      if (wait) {
         LOG(LOG_INFO, "waiting for op-thread\n");
         if (stream_wait(os)) {
            // stream_wait() now always succeeds, so this is impossible ... (?)
            LOG(LOG_ERR, "err joining op-thread ('%s')\n", strerror(errno));
            return -1;
         }
      }
   }

   // thread has completed
   os->flags |= OSF_JOINED;

   // maybe reduce the number of connections in CLOSE_WAIT, by
   // resetting connections on timed-out obj-streams.
   IOBuf*      b   = &os->iob;
   AWSContext* ctx = b->context;
   if (ctx->ch) {
      LOG(LOG_INFO, "resetting connections in curl-handle\n");
      curl_easy_cleanup(ctx->ch);
      ctx->ch = NULL;
   }


   if ((   os->flags & OSF_READING)
       && (os->op_rc == CURLE_WRITE_ERROR)) {
      // when we signalled writefunc to quit, it provoked this
      LOG(LOG_INFO, "op-thread returned CURLE_WRITE_ERROR as expected\n");
      return 0;
   }
   else {
      LOG(LOG_INFO, "op-thread returned %d\n", os->op_rc);
      // errno = (os->op_rc ? EIO : 0);
      if (os->op_rc) {
         os->flags |= OSF_THREAD_ERR;
         errno = EIO;
      }
      else
         errno = 0;

      return os->op_rc;
   }
}


// ---------------------------------------------------------------------------
// ABORT
//
// Assuming a stream is open for writing, we want a way to terminate that
// stream in such a way that the server will not think we've simply
// finished writing a legitimate object.  Instead we want it to abort
// committing this object.  This crops up in marfs ftruncate.
//
// NOTE: Co-maintain with stream_sync().
//
// NOTE: As with stream_sync(), this doesn't "close" the stream.  For that,
//       you need stream_close().  In fuse usage, we will first call
//       stream_abort(), then stream_close()
//
// NOTE: Having streaming_readfunc return CURL_READFUNC_ABORT causes the
//       The ongoing s3_put to return something other than CURLE_OKAY,
//       which means that AWS4C_CHECK1 returns 1 from stream_put.  This
//       means there's no opportunity for aws4c to parse an actual return
//       code from a server response (because there isn't a server
//       response).  We expect this situation, and validate it by looking
//       for just this case.
//
// ---------------------------------------------------------------------------

int stream_abort(ObjectStream* os) {

   // fuse may call fuse-flush multiple times (one for every open stream).
   // but will not call flush after calling close().
   if (! (os->flags & OSF_OPEN)) {
      LOG(LOG_ERR, "%s isn't open\n", os->url);
      errno = EINVAL;            /* ?? */
      return -1;
   }
   else if (! (os->flags & OSF_WRITING)) {
      LOG(LOG_ERR, "%s aborting a read-stream is not supported\n", os->url);
      errno = ENOSYS;
      return -1;
   }

   // See NOTE, above, regarding the difference between reads and writes.
   void* retval;
   if (! pthread_tryjoin_np(os->op, &retval)) {
      LOG(LOG_INFO, "op-thread joined\n");
      os->flags |= OSF_JOINED;
   }
   else {

      if (os->flags & OSF_WRITING) {
         // signal ABORT to readfunc
         LOG(LOG_INFO, "(wr) sending (char*)1 (flags=0x%04x)\n", os->flags);
         os->flags |= OSF_ABORT;
         if (stream_put(os, (const char*)1, 1) != 1) {
            LOG(LOG_ERR, "stream_put((char*)1) failed\n");
            cancel_thread(os);
         }
      }

      // check whether thread has returned.  Could mean a curl
      // error, an S3 protocol error, or server flaking out.
      LOG(LOG_INFO, "waiting for op-thread\n");
      if (stream_wait(os)) {
         // stream_wait() now always succeeds, so this is impossible ... (?)
         LOG(LOG_ERR, "err joining op-thread\n");
         return -1;
      }
   }

   // thread has completed
   os->flags |= OSF_JOINED;
   LOG(LOG_INFO, "op-thread returned %d\n", os->op_rc);

   if ((os->op_rc == CURLE_ABORTED_BY_CALLBACK)
       && (os->iob.read_pos == (char*)1)) {

      LOG(LOG_INFO, "op-thread return is as expected for ABORT\n");
      return 0;
   }
   else {
      errno = (os->op_rc ? EINVAL : 0);
      return os->op_rc;
   }
}


// ---------------------------------------------------------------------------
// CLOSE
//
// *** WARNING: This assumes stream_sync() has already been called.
//
// If marfs is implementing fuse-flush, then fuse-flush should invoke
// stream_sync() and fuse-release (aka close) should invoke stream_close().
// If fuse-flush is not implemented, then Gary says fuse falls-back to
// using fuse-release for both tasks, so fuse-close should call
// stream_sync(), then stream_close().  We separate the functions here, to
// allow either approach.
// 
// ---------------------------------------------------------------------------

int stream_close(ObjectStream* os) {

   LOG(LOG_INFO, "entry\n");
   if (! (os->flags & OSF_OPEN)) {
      LOG(LOG_ERR, "%s isn't open\n", os->url);
      errno = EINVAL;            /* ?? */
      return -1;
   }

   SEM_DESTROY(&os->iob_empty);
   SEM_DESTROY(&os->iob_full);

   os->flags &= ~(OSF_OPEN);
   os->flags |= OSF_CLOSED;     /* so stream_open() can identify re-opens */

#if 0
   // COMMENTED OUT.  Connections are apparently not getting stuck in
   // CLOSE_WAIT, anymore, and forcibly resetting like this causes open()
   // to take significantly longer, such that heavily-loaded sproxyd
   // servers more-frequently fail to respond to the Expect-100-Continue
   // within the (default) 1 second.
   //
   // If you have to uncomment this someday, consider simultaneously either
   // (a) do not add an expect-continue header, or (b) use the
   // CURLOPT_EXPECT_100_TIMEOUT_MS options (requires libcurl >= 7.38)

   // don't leave file-descriptor in CLOSE_WAIT
   LOG(LOG_INFO, "abandoning connection\n");
   aws_reset_connection_r(os->iob.context);
#endif

   int retval;
   if      ((os->op_rc == CURLE_ABORTED_BY_CALLBACK)
            && (os->flags & OSF_ABORT))
      retval = 0;
   else if ((os->op_rc == CURLE_WRITE_ERROR)
            && (os->flags & OSF_JOINED))
      retval = 0;
   else {
      errno = (os->op_rc ? EIO : 0);
      retval = os->op_rc;
   }
   LOG(LOG_INFO, "done (returning %d)\n", retval);
   return retval;
}


// (Unused.)
//
// This is the DAL-support complement of stream_init().  It's a place where
// the DAL can clean-up any per-file-handle state.  It is called after
// sync/abort and close, and has no stream-related purpose.  It's just here
// so that the expansion of "DAL_OP(destroy, ...)", in the non-DAL build,
// has something to do.

// TBD: It might make sense to move the aws_reset_connection_r() call from
//     stream_close() to here.

int stream_destroy(ObjectStream* os) {
   if (os)
      aws_iobuf_reset_hard(&os->iob);
   return 0;
}



// Reset everything except URL.  Also, use aws_iob_reset() to reset
// os->iob.
//
// NOTE: This is used when stream_open gets an OS that has been previously
//       opened and closed.  Therefore, we can assume that sems have been
//       destroyed, and thread has been joined or killed.
//
void stream_reset(ObjectStream* os,
                  uint8_t       preserve_os_written) {
   if (! (os->flags & OSF_CLOSED)) {
      LOG(LOG_ERR, "We require a stream that was previously opened\n");
      return;
   }

#if 0
   char*  before_ptr  = (char*)os;
   size_t before_size = (char*)os->url - (char*)os;

   char*  after_ptr   = (char*)os->url + MARFS_MAX_URL_SIZE;
   size_t after_size  = (char*)os + sizeof(ObjectStream) - after_ptr;

   memset(before_ptr, 0, before_size);
   memset(after_ptr,  0, after_size);

#else
   aws_iobuf_reset(&os->iob);
   os->op_rc      = 0;
   os->flags      = 0;
   // os->open_flags = 0;

   if (! preserve_os_written)
      os->written = 0;
#endif
}


// ---------------------------------------------------------------------------
// DELETE
//
// Unlike all the functions above, this one is not really a "stream"
// function.  We're just deleting the object with the given ID.
//
// NOTE: s3_delete() expects host and bucket to be installed in the
//    AWSContext, and the "object-id" argument to be just the part after
//    the bucket.  That's the information we keep in the Pre component of a
//    MarFS file-handle.
// ---------------------------------------------------------------------------

#if 0

// If caller gave us the file-handle, we'd already have host, bucket, and
// objid (not including bucket) parsed out separately, in the info.pre
// struct.  However, it seems like the stream support should stick with
// ObjectStreams.  So, we re-parse those fields out separately, here.  This
// implies that caller already did update_pre(&fh.info.pre), and
// update_url(&fh.os, &fh.info), before calling this.
//
// UPDATE: we have made update_url() a DAL interface function (and
// renamed it "update_object_location"). Any caller should be sure to
// call that first as described above.

int stream_del(ObjectStream* os) {
   LOG(LOG_INFO, "URL: %s\n", os->url);

   if (os->flags & OSF_OPEN) {
      LOG(LOG_INFO, "%s is open (for %s)\n",
          os->url, ((os->flags & OSF_WRITING) ? "writing" : "reading"));
      TRY0( stream_sync(os) );
      TRY0( stream_close(os) );
   }

   IOBuf*      b   = os->iob;   // shorthand
   AWSContext* ctx = b->ctx;    // shorthand

   // parse "host", "bucket", and "objid" (the part after bucket), from the
   // URL in the OS.  We ultimately need them to be distinct tokens.  Seems
   // simplest to just copy the whole URL, and parse the tokens
   // destructively, in place.  Ugh.
   char URL2[MARFS_MAX_URL_SIZE];
   strcpy(URL2, os->url);
   char* url = URL2;
   char* state = NULL;

   const char* delims = "/";
   char* http   = strtok_r(url,  delims, &state);
   char* host   = strtok_r(NULL, delims, &state);
   char* bucket = strtok_r(NULL, delims, &state);
   char* obj    = strtok_r(NULL, delims, &state);

   LOG(LOG_INFO, "DEL  '%s/%s/%s'\n", host, bucket, obj);
   if (! (host && bucket && obj)) {
      LOG(LOG_ERR, "parse failed\n");
      errno = ENOENT;           // "no such file"
      return -1;
   }

   // delete the object
   s3_set_host_r(host, ctx);
   s3_set_bucket_r(bucket, ctx);
   AWS4C_CHECK1( s3_delete(b, obj) );

   return 0;
}
#endif



// The non-DAL build calls this.  We can't get host, bucket, etc, from the
// os (they are in info.pre component of the file-handle), but they will
// already have been installed into the AWS context in stream_init(), so
// they are redundant.
int     stream_del(ObjectStream* os) {

   LOG(LOG_INFO, "URL: %s\n", os->url);

#if 0
   // parse "host", "bucket", and "objid" (the part after bucket), from the
   // URL in the OS.  We ultimately need them to be distinct tokens.  Seems
   // simplest to just copy the whole URL, and parse the tokens
   // destructively, in place.  Ugh.
   char URL2[MARFS_MAX_URL_SIZE];
   strcpy(URL2, os->url);
   char* url = URL2;
   char* state = NULL;

   const char* delims = "/";
   __attribute__ ((unused)) char* http   = strtok_r(url,  delims, &state);
   char* host   = strtok_r(NULL, delims, &state);
   char* bucket = strtok_r(NULL, delims, &state);
   char* obj    = strtok_r(NULL, delims, &state);

   if (! (host && bucket && obj_name)) {
      LOG(LOG_ERR, "parse failed\n");
      errno = ENOENT;           // "no such file"
      return -1;
   }

   return stream_del_components(os, host, bucket, obj);

#else
   return stream_del_components(os, NULL, NULL, os->url);

#endif
}



// The "OBJECT" DAL calls this, providing host, bucket and obj_name.
//
// This approach allows callers at the MarFS_FileHandle level to just give
// us the arguments we want, rather than making us reparse them.

int     stream_del_components(ObjectStream* os,
                              const char*   host,
                              const char*   bucket,
                              const char*   obj_name) {
   TRY_DECLS();
   LOG(LOG_INFO, "DEL  '%s/%s/%s'\n", host, bucket, obj_name);

   if (os->flags & OSF_OPEN) {
      LOG(LOG_INFO, "%s is open (for %s)\n",
          os->url, ((os->flags & OSF_WRITING) ? "writing" : "reading"));
      TRY0( stream_sync(os) );
      TRY0( stream_close(os) );
   }

   IOBuf*      b   = &os->iob;   // shorthand
   AWSContext* ctx = b->context; // shorthand

   // delete the object
   if (host)
      s3_set_host_r(host, ctx);     // stream_init() already did this
   if (bucket)
      s3_set_bucket_r(bucket, ctx); // stream_init() already did this

   AWS4C_CHECK1( s3_delete(b, (char* const)obj_name) );

   return 0;
}


// We might be reopening an object stream that was previously used.
// This happens, for example, when we overwrite a file.
//
// This is a piece of code that needs to be executed at the begining
// of every call to DAL->open() in order to guard against reopening a
// previously used object stream and clean up the flags if that is the
// case.
//
// Returns 0 on success or -1 on error and sets errno = EBADF.
int stream_cleanup_for_reopen(ObjectStream* os, int preserve_write_count) {
   ENTRY();

   if(os->flags) {
      if(os->flags & OSF_CLOSED) {
         LOG(LOG_INFO, "previously used OS: %s. resetting flags.\n",
             os->url);
         os->flags = 0;
         if(! preserve_write_count)
            os->written = 0;
      }
      // Guard against the O_RDONLY case. RLOCK_INIT will be asserted
      // but that doesn't mean the OS is invalid, just that we ore
      // opening for reading.
      else if(! (os->flags & ~OSF_RLOCK_INIT)) {
         LOG(LOG_INFO, "only flag was RLOCK_INIT\n");
      }
      else {
         LOG(LOG_ERR, "%s has flags asserted, but is not CLOSED.\n", os->url);
         errno = EBADF;
         return -1;
      }
   }

   EXIT();
   return 0;
}
