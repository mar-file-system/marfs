

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Installing &mdash; MarFS 1.13 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script type="text/javascript" src="_static/jquery.js"></script>
        <script type="text/javascript" src="_static/underscore.js"></script>
        <script type="text/javascript" src="_static/doctools.js"></script>
        <script type="text/javascript" src="_static/language_data.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Developers" href="developers.html" />
    <link rel="prev" title="Welcome to MarFS’s documentation!" href="index.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> MarFS
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Installing</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#overview">Overview</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#storage">Storage</a></li>
<li class="toctree-l3"><a class="reference internal" href="#data-access">Data Access</a></li>
<li class="toctree-l3"><a class="reference internal" href="#data-movement">Data Movement</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#example-cluster-summary">Example cluster summary</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#storage-nodes">Storage Nodes</a></li>
<li class="toctree-l3"><a class="reference internal" href="#metadata-nodes">Metadata Nodes</a></li>
<li class="toctree-l3"><a class="reference internal" href="#file-transfer-nodes">File Transfer Nodes</a></li>
<li class="toctree-l3"><a class="reference internal" href="#interactive-nodes">Interactive Nodes</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#marfs-abstractions">MarFS abstractions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#the-repository">The Repository</a></li>
<li class="toctree-l3"><a class="reference internal" href="#data-abstraction-layer">Data Abstraction Layer</a></li>
<li class="toctree-l3"><a class="reference internal" href="#metadata-abstraction-layer">Metadata Abstraction Layer</a></li>
<li class="toctree-l3"><a class="reference internal" href="#the-namespace">The Namespace</a></li>
<li class="toctree-l3"><a class="reference internal" href="#pods">Pods</a></li>
<li class="toctree-l3"><a class="reference internal" href="#blocks">Blocks</a></li>
<li class="toctree-l3"><a class="reference internal" href="#capacity-units">Capacity Units</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#dependencies">Dependencies</a></li>
<li class="toctree-l2"><a class="reference internal" href="#setup-process">Setup Process</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id1">Storage Nodes</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id2">Metadata Nodes</a></li>
<li class="toctree-l3"><a class="reference internal" href="#fta-nodes">FTA nodes</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#building-software-for-marfs">Building software for MarFS</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="developers.html">Developers</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">MarFS</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>Installing</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/install.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="installing">
<h1>Installing<a class="headerlink" href="#installing" title="Permalink to this headline">¶</a></h1>
<p>This section describes the install process for MarFS.</p>
<div class="contents local topic" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#overview" id="id3">Overview</a></p>
<ul>
<li><p><a class="reference internal" href="#storage" id="id4">Storage</a></p></li>
<li><p><a class="reference internal" href="#data-access" id="id5">Data Access</a></p></li>
<li><p><a class="reference internal" href="#data-movement" id="id6">Data Movement</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#example-cluster-summary" id="id7">Example cluster summary</a></p>
<ul>
<li><p><a class="reference internal" href="#storage-nodes" id="id8">Storage Nodes</a></p></li>
<li><p><a class="reference internal" href="#metadata-nodes" id="id9">Metadata Nodes</a></p></li>
<li><p><a class="reference internal" href="#file-transfer-nodes" id="id10">File Transfer Nodes</a></p></li>
<li><p><a class="reference internal" href="#interactive-nodes" id="id11">Interactive Nodes</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#marfs-abstractions" id="id12">MarFS abstractions</a></p>
<ul>
<li><p><a class="reference internal" href="#the-repository" id="id13">The Repository</a></p></li>
<li><p><a class="reference internal" href="#data-abstraction-layer" id="id14">Data Abstraction Layer</a></p></li>
<li><p><a class="reference internal" href="#metadata-abstraction-layer" id="id15">Metadata Abstraction Layer</a></p></li>
<li><p><a class="reference internal" href="#the-namespace" id="id16">The Namespace</a></p></li>
<li><p><a class="reference internal" href="#pods" id="id17">Pods</a></p></li>
<li><p><a class="reference internal" href="#blocks" id="id18">Blocks</a></p></li>
<li><p><a class="reference internal" href="#capacity-units" id="id19">Capacity Units</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#dependencies" id="id20">Dependencies</a></p></li>
<li><p><a class="reference internal" href="#setup-process" id="id21">Setup Process</a></p>
<ul>
<li><p><a class="reference internal" href="#id1" id="id22">Storage Nodes</a></p></li>
<li><p><a class="reference internal" href="#id2" id="id23">Metadata Nodes</a></p></li>
<li><p><a class="reference internal" href="#fta-nodes" id="id24">FTA nodes</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#building-software-for-marfs" id="id25">Building software for MarFS</a></p></li>
</ul>
</div>
<div class="section" id="overview">
<h2><a class="toc-backref" href="#id3">Overview</a><a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h2>
<p>MarFS is a distributed parallel filesystem, thusly setup is complicated and
relies on a number of different technologies and systems. In this guide we
will go over the setup of an example system. This guide assumes knowledge of
ZFS, GPFS. MPI, and general Linux operations.</p>
<div class="section" id="storage">
<h3><a class="toc-backref" href="#id4">Storage</a><a class="headerlink" href="#storage" title="Permalink to this headline">¶</a></h3>
<p>MarFS stores data and metadata differently. Data is stored as erasure-coded
objects. When data is written it is broken up into N number of pieces. Erasure
data is calculated on our N objects to create E number of erasure objects.
N+E number of objects are then mapped onto N+E identical filesystems. We refer
to these N+E filesystems as a “pod”. In our example cluster we will have four
storage nodes in a single pod giving us a 3+1 erasure coding scheme. You can
have multiple pods in a cluster. When you have multiple pods the pod is
selected with a hash, and data is written to that pod. Data will never be
written across multiple pods. So if you have 4 pods each matching our single
pod with a 3+1 scheme those four objects will always be in the same pod.</p>
<p>Metadata can be stores on any filesystem that supports extended attributes and
sparse-files. For scalability purposes a distributed filesystem is highly
recommended. In our example cluster we will use two nodes running General
Parallel Filesystem (GPFS).</p>
</div>
<div class="section" id="data-access">
<h3><a class="toc-backref" href="#id5">Data Access</a><a class="headerlink" href="#data-access" title="Permalink to this headline">¶</a></h3>
<p>With object data being stored across a number of pods it is reasonable to
provide a way to interact with the filesystem in a unified matter. Most users
would expect a single mount point they can look through for various tasks.
This is provided through FUSE, allowing users to look at their data. This
FUSE mount is read only, and is there for users to locate their files for
parallel movement. Nodes with this FUSE mount are referred to as “interactive”
nodes. Interactive nodes are unique in the MarFS cluster, as it is the only
node users will have direct access.</p>
</div>
<div class="section" id="data-movement">
<h3><a class="toc-backref" href="#id6">Data Movement</a><a class="headerlink" href="#data-movement" title="Permalink to this headline">¶</a></h3>
<p>Data is moved in parallel using PFTool. Nodes running PFTool are called
“File Transfer Agent” nodes, or FTAs.</p>
</div>
</div>
<div class="section" id="example-cluster-summary">
<h2><a class="toc-backref" href="#id7">Example cluster summary</a><a class="headerlink" href="#example-cluster-summary" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ul class="simple">
<li><p>4 Storage Nodes</p></li>
<li><p>2 Metadata Nodes</p></li>
<li><p>2 File Transfer Nodes</p></li>
<li><p>1 Interactive Node</p></li>
</ul>
</div></blockquote>
<div class="section" id="storage-nodes">
<h3><a class="toc-backref" href="#id8">Storage Nodes</a><a class="headerlink" href="#storage-nodes" title="Permalink to this headline">¶</a></h3>
<p>Each storage node uses ZFS for MarFS block storage. Each node will have four
zpools in a RAIDZ3(17+3) configuration. We have a single pod configured to
use 3+1 erasure coding. Must have high performance network such as Infiniband.</p>
</div>
<div class="section" id="metadata-nodes">
<h3><a class="toc-backref" href="#id9">Metadata Nodes</a><a class="headerlink" href="#metadata-nodes" title="Permalink to this headline">¶</a></h3>
<p>We will be using GPFS as metadata storage in this example. Your GPFS cluster
should already be setup and ready to create filesets. You can still follow the
example using some other filesystem. Should have high performance network such
as Infiniband when using GPFS. It is important to note that while GPFS is not
required to use MarFS it is required for some MarFS utilities like quota
management and garbage collection.</p>
</div>
<div class="section" id="file-transfer-nodes">
<h3><a class="toc-backref" href="#id10">File Transfer Nodes</a><a class="headerlink" href="#file-transfer-nodes" title="Permalink to this headline">¶</a></h3>
<p>These nodes will be used to move data in parallel from one place to another.
We will use <a class="reference external" href="https://github.com/pftool/pftool">PFtool</a> for this.
Must have high performance network such as Infiniband.</p>
</div>
<div class="section" id="interactive-nodes">
<h3><a class="toc-backref" href="#id11">Interactive Nodes</a><a class="headerlink" href="#interactive-nodes" title="Permalink to this headline">¶</a></h3>
<p>These nodes will be used to present MarFS to users through a FUSE mount.</p>
</div>
</div>
<div class="section" id="marfs-abstractions">
<h2><a class="toc-backref" href="#id12">MarFS abstractions</a><a class="headerlink" href="#marfs-abstractions" title="Permalink to this headline">¶</a></h2>
<p>Remember how earlier we talked about the pod? There are more things to
understand about the pod. There are logical data abstractions we will see
later when understanding the configuration file. We will talk about them
briefly here first.</p>
<div class="section" id="the-repository">
<h3><a class="toc-backref" href="#id13">The Repository</a><a class="headerlink" href="#the-repository" title="Permalink to this headline">¶</a></h3>
<p>A repo is where all the object data for a MarFS Filesystem lives; it’s a
logical description of a MarFS object-store, with details on the number of
storage servers, etc.</p>
<p>#The repo currently includes configuration details
#specific to MC-NFS versus MC-RDMA.</p>
</div>
<div class="section" id="data-abstraction-layer">
<h3><a class="toc-backref" href="#id14">Data Abstraction Layer</a><a class="headerlink" href="#data-abstraction-layer" title="Permalink to this headline">¶</a></h3>
<p>Multi component stuff here maybe?</p>
</div>
<div class="section" id="metadata-abstraction-layer">
<h3><a class="toc-backref" href="#id15">Metadata Abstraction Layer</a><a class="headerlink" href="#metadata-abstraction-layer" title="Permalink to this headline">¶</a></h3>
</div>
<div class="section" id="the-namespace">
<h3><a class="toc-backref" href="#id16">The Namespace</a><a class="headerlink" href="#the-namespace" title="Permalink to this headline">¶</a></h3>
<p>A namespace in MarFS is a logical partition of the MarFS filesystem with a
unique (virtual) mount point and attributes like permissions, similar to ZFS
datasets. It also includes configuration details regarding MarFS metadata
storage for that namespace.  Each namespace in MarFS must be associated with a
repo, and you can have multiple namespaces per repo. Both repos and namespaces
are arbitrarily named.</p>
</div>
<div class="section" id="pods">
<h3><a class="toc-backref" href="#id17">Pods</a><a class="headerlink" href="#pods" title="Permalink to this headline">¶</a></h3>
<p>A collection of storage nodes.</p>
</div>
<div class="section" id="blocks">
<h3><a class="toc-backref" href="#id18">Blocks</a><a class="headerlink" href="#blocks" title="Permalink to this headline">¶</a></h3>
<p>A storage node in a pod.</p>
</div>
<div class="section" id="capacity-units">
<h3><a class="toc-backref" href="#id19">Capacity Units</a><a class="headerlink" href="#capacity-units" title="Permalink to this headline">¶</a></h3>
<p>Each capacity unit (cap) is a datastore on a ZFS zpool on a block in a pod :)</p>
</div>
</div>
<div class="section" id="dependencies">
<h2><a class="toc-backref" href="#id20">Dependencies</a><a class="headerlink" href="#dependencies" title="Permalink to this headline">¶</a></h2>
<p>Depending on things you may need different things. To install and make use of
MarFS you will need the following tools.</p>
<p>Fortunately many dependencies can be acquired through a package manager.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>yum install gcc glibc-devel fuse-devel libattr-devel make curl-devel
curl openssl-devel openssl git libxml2-devel yasm libtool openmpi
openmpi-devel
</pre></div>
</div>
<p>Others can be obtained from source.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>git clone https://github.com/mar-file-system/marfs.git
git clone https://github.com/mar-file-system/PA2X.git
git clone https://github.com/mar-file-system/erasureUtils.git
git clone https://github.com/mar-file-system/aws4c.git
git clone https://github.com/pftool/pftool.git
git clone https://github.com/01org/isa-l.git
</pre></div>
</div>
<p>A quick description of tools acquired from source:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>MarFS: The core MarFS libraries
PA2X: An XML parser for parsing the MarFS configuration file
ErasureUtils: The erasure coding layer used for Multi-Component storage
Aws4c: C library for AWS, used for S3 and RDMA authentication
Pftool: A tool for parallel data movement
ISA-L: Intel’s Intelligent Storage Acceleration Library
</pre></div>
</div>
</div>
<div class="section" id="setup-process">
<h2><a class="toc-backref" href="#id21">Setup Process</a><a class="headerlink" href="#setup-process" title="Permalink to this headline">¶</a></h2>
<p>You will need yasm 1.2.0 or later for ISA-L.</p>
<p>It is helpful to have a shared filesystem among all the nodes in the cluster,
in this guide we will have a NFS share mounted on all nodes. We will keep all
our source code and other files that must be shared here.</p>
<p>For machines that have Infiniband:
Ensure MPI is in your <code class="code docutils literal notranslate"><span class="pre">$PATH</span></code> environment variable.
It may also be required to add OpenMPI’s library directory to the
<code class="code docutils literal notranslate"><span class="pre">$LD_LIBRARY_PATH</span></code> environment variable.</p>
<p>Your metadata nodes and FTA nodes should all be in a GPFS cluster that is set
up.</p>
<p>Your storage nodes should all have ZFS installed, with your zpools set up.</p>
<p>Our example cluster will have a single pod containing four blocks. Each block
will have four capacity units.
In human terms, we have one set of storage servers comprised of four storage
servers. Each of these storage servers will have four ZFS zpools set up.</p>
<div class="section" id="id1">
<h3><a class="toc-backref" href="#id22">Storage Nodes</a><a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<p>MarFS object data is stored in zpools on each storage node. The path to the
objects must match a pattern similar to
<code class="code docutils literal notranslate"><span class="pre">FTAMountPoint/RepoName/podNum/blockNum/capNum</span></code>
examle:
<code class="code docutils literal notranslate"><span class="pre">/zfs/repo3+1/pod0/block0/cap3</span></code>
This path corresponds to storage pool number 3 on storage node 0 in pod 0 in
repo “repo3+1”.
On storage nodes this path matching is not required. The data can actually be
stored in any arbitrary directory. On FTA nodes that path structure is
required, as the MarFS library is hard coded to use that path. We will be
using the same path on our storage nodes for symmetry between the FTA nodes
and storage nodes. Each storage node will only need the unique path that
corresponds to the capacity units. Hostnames are arbitrary, but can help in
the brain battle of keeping things oraginzed. Our hostnames for storage nodes
will be:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">sn01</span>
<span class="n">sn02</span>
<span class="n">sn03</span>
<span class="n">sn04</span>
</pre></div>
</div>
<p>We’ll start with sn01:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="o">[</span>sn01 ~<span class="o">]</span><span class="c1"># zpool list</span>
NAME             SIZE  ALLOC   FREE  CKPOINT  EXPANDSZ   FRAG    CAP  DEDUP    HEALTH  ALTROOT
sn01-pool0       146T  <span class="m">12</span>.7M   146T        -         -     <span class="m">0</span>%     <span class="m">0</span>%  <span class="m">1</span>.00x    ONLINE  -
sn01-pool1       146T  <span class="m">11</span>.0M   146T        -         -     <span class="m">0</span>%     <span class="m">0</span>%  <span class="m">1</span>.00x    ONLINE  -
sn01-pool2       146T  <span class="m">10</span>.8M   146T        -         -     <span class="m">0</span>%     <span class="m">0</span>%  <span class="m">1</span>.00x    ONLINE  -
sn01-pool3       146T  <span class="m">11</span>.0M   146T        -         -     <span class="m">0</span>%     <span class="m">0</span>%  <span class="m">1</span>.00x    ONLINE  -
</pre></div>
</div>
<p>First we want to set the optimal zpool settings on all our zpools.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> i in <span class="o">{</span><span class="m">0</span>..3<span class="o">}</span><span class="p">;</span> <span class="k">do</span> zfs <span class="nb">set</span> <span class="nv">recordsize</span><span class="o">=</span>1M sn01-pool<span class="nv">$i</span><span class="p">;</span> <span class="k">done</span>
<span class="k">for</span> i in <span class="o">{</span><span class="m">0</span>..3<span class="o">}</span><span class="p">;</span> <span class="k">do</span> zfs <span class="nb">set</span> <span class="nv">mountpoint</span><span class="o">=</span>none sn01-pool<span class="nv">$i</span><span class="p">;</span> <span class="k">done</span>
<span class="k">for</span> i in <span class="o">{</span><span class="m">0</span>..3<span class="o">}</span><span class="p">;</span> <span class="k">do</span> zfs <span class="nb">set</span> <span class="nv">compression</span><span class="o">=</span>lz4 sn01-pool<span class="nv">$i</span><span class="p">;</span> <span class="k">done</span>
<span class="k">for</span> i in <span class="o">{</span><span class="m">0</span>..3<span class="o">}</span><span class="p">;</span> <span class="k">do</span> zfs <span class="nb">set</span> <span class="nv">atime</span><span class="o">=</span>off sn01-pool<span class="nv">$i</span><span class="p">;</span> <span class="k">done</span>
</pre></div>
</div>
<p>We are using a diskless sever for our storage nodes. We need to create a NFS
exported ZFS datastore, with the mountpoint at <code class="code docutils literal notranslate"><span class="pre">/zfs</span></code>. This datastore
must be mounted before all the others on reboot because NFS will stat the
mountpoint which is on <code class="code docutils literal notranslate"><span class="pre">tmpfs</span></code> in a diskless setup. When it does the
stat the wrong block size will be returned.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>zfs create sn01-pool0/nfs
zfs <span class="nb">set</span> <span class="nv">mountpoint</span><span class="o">=</span>/zfs sn01-pool0/nfs
</pre></div>
</div>
<p>We want a datastore on each zpool that will be mounted at a path made with the
above guidelines. The name of the datastore is irrelevant.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> i in <span class="o">{</span><span class="m">0</span>..3<span class="o">}</span><span class="p">;</span> <span class="k">do</span> zfs create sn002-pool<span class="nv">$i</span>/datastore<span class="p">;</span> <span class="k">done</span>
</pre></div>
</div>
<p>On each storage node we want to make a directory under our /zfs mountpoint
where we will create out special path</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mkdir /zfs/exports
</pre></div>
</div>
<p>Now we want to make our <code class="code docutils literal notranslate"><span class="pre">pod/block/cap</span></code> directories
under <code class="code docutils literal notranslate"><span class="pre">/zfs/exports</span></code>. For sn01 it looks like:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mkdir /zfs/exports/repo3+1/pod0/block0/cap0
mkdir /zfs/exports/repo3+1/pod0/block0/cap1
mkdir /zfs/exports/repo3+1/pod0/block0/cap2
mkdir /zfs/exports/repo3+1/pod0/block0/cap3
</pre></div>
</div>
<p>Storage node sn01 is in pod 0, is block 0 of the pod, and will have 4 capacity
units. We will want to create the correct path on every storage node in the
cluster. For sn02 it would look like:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mkdir /zfs/exports/repo3+1/pod0/block1/cap0
mkdir /zfs/exports/repo3+1/pod0/block1/cap1
mkdir /zfs/exports/repo3+1/pod0/block1/cap2
mkdir /zfs/exports/repo3+1/pod0/block1/cap3
</pre></div>
</div>
<p>For loops are very helpful for this with minor adjustments on each node.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> i in <span class="o">{</span><span class="m">0</span>..3<span class="o">}</span><span class="p">;</span> <span class="k">do</span> mkdir -p /zfs/exports/repo3+1/pod0/block3/cap<span class="nv">$i</span>
</pre></div>
</div>
<p>All you need to do is change the pod and block to the correct number for each
storage node. If everything is in sequence you can just wrap that loop in
more loops to handle that with SSH. After we create the directories we need,
we will mount our datastores on each node into the correct folder. on sn01 it
will look like:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="o">[</span>sn01 ~<span class="o">]</span><span class="c1"># zfs list</span>
NAME                       USED  AVAIL     REFER  MOUNTPOINT
sn01-pool0                 <span class="m">9</span>.29M 113T      307K   none
sn01-pool0/datastore       <span class="m">5</span>.14M 113T      <span class="m">5</span>.14M  /zfs/exports/repo3+1/pod0/block0/cap0
sn01-pool0/nfs             332K  113T      332K   /zfs
sn01-pool1                 <span class="m">8</span>.44M 113T      307K   none
sn01-pool1/datastore       <span class="m">5</span>.14M 113T      <span class="m">5</span>.14M  /zfs/exports/repo3+1/pod0/block0/cap1
sn01-pool2                 <span class="m">8</span>.25M 113T      307K   none
sn01-pool2/datastore       <span class="m">5</span>.14M 113T      <span class="m">5</span>.14M  /zfs/exports/repo3+1/pod0/block0/cap2
sn01-pool3                 <span class="m">8</span>.40M 113T      307K   none
sn01-pool3/datastore       <span class="m">5</span>.14M 113T      <span class="m">5</span>.14M  /zfs/exports/repo3+1/pod0/block0/cap3
</pre></div>
</div>
<p>Once we have our capacity units mounted we must create “scatter” directories
under the mount point for each capacity unit.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> c in <span class="o">{</span><span class="m">0</span>..3<span class="o">}</span><span class="p">;</span> <span class="k">do</span>
   <span class="k">for</span> s in <span class="o">{</span><span class="m">0</span>..1024<span class="o">}</span><span class="p">;</span> <span class="k">do</span>
      mkdir /zfs/exports/repo3+1/pod0/block0/cap<span class="nv">$c</span>/scatter<span class="nv">$s</span>
   <span class="k">done</span>
<span class="k">done</span>
</pre></div>
</div>
<p>The purpose of these directories is just to prevent all objects destined for a
particular capacity-dir from being stored in a single-directory. The specific
scatter-dir used for each object is computed at run-time by a hash. In our
example we will only create 1024 scatter directories, but in bigger systems
you can have many more.</p>
<p>Now we can NFS export out datasets. Edit the file <code class="code docutils literal notranslate"><span class="pre">/etc/exports</span></code> to
look like:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="o">[</span>sn01 ~<span class="o">]</span><span class="c1"># cat /etc/exports</span>
/zfs/exports *<span class="o">(</span>rw,fsid<span class="o">=</span><span class="m">0</span>,no_subtree_check,sync,crossmnt<span class="o">)</span>
</pre></div>
</div>
<p><em>Important</em>
If you plan on using NFS over RDMA (you should) you will need to change the
export options in <code class="code docutils literal notranslate"><span class="pre">/etc/exports</span></code>:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="o">[</span>sn01 ~<span class="o">]</span><span class="c1"># cat /etc/exports</span>
/zfs/exports *<span class="o">(</span>rw,fsid<span class="o">=</span><span class="m">0</span>,no_root_squash,no_subtree_check,sync,insecure,crossmnt<span class="o">)</span>
</pre></div>
</div>
<p>NFS over RDMA requires the extra options.</p>
</div>
<div class="section" id="id2">
<h3><a class="toc-backref" href="#id23">Metadata Nodes</a><a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h3>
<p>Phew we made it. Now that the easy part is over we will configure our metadata
nodes with GPFS and get them ready to hold metadata. Just kidding. I made you
do all the hard work for metadata nodes on GPFS way before now.</p>
<p>We have our GPFS filesystem all set up and mounted under <code class="code docutils literal notranslate"><span class="pre">/gpfs</span></code>.
Create a directory <code class="code docutils literal notranslate"><span class="pre">mkdir</span> <span class="pre">/gpfs/metadata</span></code>. We will create a GPFS fileset
for each namespace that we want to create in our config file. In this example
we will have a single namespace. We will link our filesets under the directory
we made. We will create some directories we need under those links.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mmcrfileset /dev/marfs namespace_one
mmlinkfileset /dev/marfs namespace_one -J /gpfs/metadata/namespace_one
mkdir /gpfs/metadata/namespace_one/mdfs
</pre></div>
</div>
<p>All MDFS directories should be readable by everyone. We also want to set
ownership of the MDFS directory here. The permissions and ownership of this
directory will be reflected as the permissions in the MarFS mount later. So
<code class="code docutils literal notranslate"><span class="pre">chown</span></code> this directory to the right group now if needed.</p>
<p>There is a file MarFS will always look for under the mdfs directory called
<code class="code docutils literal notranslate"><span class="pre">fsinfo</span></code>. Lets create that now.</p>
<p><code class="code docutils literal notranslate"><span class="pre">touch</span> <span class="pre">/gpfs/metadata/namespace_one/mdfs/fsinfo</span></code></p>
</div>
<div class="section" id="fta-nodes">
<h3><a class="toc-backref" href="#id24">FTA nodes</a><a class="headerlink" href="#fta-nodes" title="Permalink to this headline">¶</a></h3>
<p>Now that we have storage and metadata all up and running we have to unite the
two systems so we can read and write data. FTA nodes will have the capacity
units mounted that we exported earlier, and should have the metadata
filesystem mounted as well. The FTAs in our setup are part of the GPFS cluster
so we should already have /gpfs mounted on these nodes. We need to use the
<code class="code docutils literal notranslate"><span class="pre">pod/block/cap/</span></code> directory tree we created earlier on the storage nodes.
We already have our metadata mounted at <code class="code docutils literal notranslate"><span class="pre">/gpfs/metadata</span></code> so lets create
a new directory to hold the <code class="code docutils literal notranslate"><span class="pre">pod/block/cap/</span></code> structure.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mkdir /gpfs/repo_data

<span class="k">for</span> b in <span class="o">{</span><span class="m">0</span>..3<span class="o">}</span><span class="p">;</span> <span class="k">do</span>
   <span class="k">for</span> c in <span class="o">{</span><span class="m">0</span>..3<span class="o">}</span><span class="p">;</span> <span class="k">do</span>
      mkdir -p /gpfs/data/repo3+1/pod0/block<span class="nv">$b</span>/cap<span class="nv">$c</span><span class="p">;</span> <span class="k">done</span><span class="p">;</span> <span class="k">done</span>
</pre></div>
</div>
<p>Behold. Now mount the datastores.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mount -o <span class="s2">&quot;_netdev,async,rw,nfsvers=3,nolock,wsize=1048576,rsize=1048576,rdma,soft,port=20049&quot;</span> -t nfs <span class="se">\</span>
         sn01:/zfs/exports/repo3+1/pod0/block0/cap0 /gpfs/data/repo3+1/pod0/block0/cap0
</pre></div>
</div>
<p>Do that for all capacity units with the correct pod block and cap numbers.</p>
<p>Once that has been done we’re ready to build our software dependencies.</p>
</div>
</div>
<div class="section" id="building-software-for-marfs">
<h2><a class="toc-backref" href="#id25">Building software for MarFS</a><a class="headerlink" href="#building-software-for-marfs" title="Permalink to this headline">¶</a></h2>
<p>That’s right. That was just what we needed to do to be able to build MarFS.</p>
<img alt="_images/wheeze.gif" src="_images/wheeze.gif" />
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="developers.html" class="btn btn-neutral float-right" title="Developers" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="index.html" class="btn btn-neutral float-left" title="Welcome to MarFS’s documentation!" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020 Triad National Security, LLC

    </p>
  </div> 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>